{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bREDkm-AkDms",
   "metadata": {
    "id": "bREDkm-AkDms"
   },
   "source": [
    "# <b><u> Project Title : Live Class Monitoring System - Face emotion Recognition</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U7DoJ9GekGnx",
   "metadata": {
    "id": "U7DoJ9GekGnx"
   },
   "source": [
    "## <b> Problem Description </b>\n",
    "\n",
    "The Indian education landscape has been undergoing rapid changes for the past 10 years owing to\n",
    "the advancement of web-based learning services, specifically, eLearning platforms.\n",
    "\n",
    "Global E-learning is estimated to witness an 8X over the next 5 years to reach USD 2B in 2021. India\n",
    "is expected to grow with a CAGR of 44% crossing the 10M users mark in 2021. Although the market\n",
    "is growing on a rapid scale, there are major challenges associated with digital learning when\n",
    "compared with brick and mortar classrooms. One of many challenges is how to ensure quality\n",
    "learning for students. Digital platforms might overpower physical classrooms in terms of content\n",
    "quality but when it comes to understanding whether students are able to grasp the content in a live\n",
    "class scenario is yet an open-end challenge.\n",
    "\n",
    "\n",
    "In a physical classroom during a lecturing teacher can see the faces and assess the emotion of the\n",
    "class and tune their lecture accordingly, whether he is going fast or slow. He can identify students who\n",
    "need special attention. Digital classrooms are conducted via video telephony software program (exZoom) where it’s not possible for medium scale class (25-50) to see all students and access the\n",
    "mood. Because of this drawback, students are not focusing on content due to lack of surveillance.\n",
    "While digital platforms have limitations in terms of physical surveillance but it comes with the power of\n",
    "data and machines which can work for you. It provides data in the form of video, audio, and texts\n",
    "which can be analysed using deep learning algorithms. Deep learning backed system not only solves\n",
    "the surveillance issue, but it also removes the human bias from the system, and all information is no\n",
    "longer in the teacher’s brain rather translated in numbers that can be analysed and tracked.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We will solve the above-mentioned challenge by applying deep learning algorithms to live video data.\n",
    "The solution to this problem is by recognizing facial emotions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CqcEc1AnkP2M",
   "metadata": {
    "id": "CqcEc1AnkP2M"
   },
   "source": [
    "## <b> Data Description </b>\n",
    "\n",
    "### <b> FER-2013 Data\n",
    "\n",
    "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image.\n",
    "\n",
    "The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ACM9BaHvkkQh",
   "metadata": {
    "id": "ACM9BaHvkkQh"
   },
   "source": [
    " # Data Pre-Processing and Sequential CNN Model Building :\n",
    " \n",
    " # Data Pre-Processing\n",
    "\n",
    "\n",
    "It Contains several steps like :\n",
    "\n",
    "\n",
    "- Data Collection\n",
    "- Data Import\n",
    "- Data Inspection\n",
    "- Data Splitting\n",
    "- Resizing image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D7Pr_MtKly18",
   "metadata": {
    "id": "D7Pr_MtKly18"
   },
   "source": [
    "# Feature Extraction\n",
    "\n",
    "##### In Traditional Machine Learning\n",
    " Input Image >> Feature Selections >> Classifier >> Result \n",
    "\n",
    "##### In Deep Learning\n",
    " Input Image >> Neural Network >> Result\n",
    "\n",
    "   #### For CNN\n",
    " Input Layer >> Hidden Layers >> Output\n",
    ">> Here, input layer takes the input and output gives the desired output.\n",
    ">> That means hidden layer is doing some feature extractions.\n",
    ">> And if we extract the outputs of the hidden layers, then we will get different features. \n",
    "\n",
    "So basically, In ML we have to create our own feature vector, \n",
    "while for DL algorithms they extract features automatically in convolution layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hkFXgiNhmE3z",
   "metadata": {
    "id": "hkFXgiNhmE3z"
   },
   "source": [
    "#### A CNN is composed of two basic parts of feature extraction and classification. \n",
    "\n",
    "- Feature extraction includes several convolution layers followed by max-pooling and an activation function.\n",
    "\n",
    "\n",
    "- The classifier usually consists of fully connected layers. \n",
    "\n",
    "\n",
    "- CNN automatically detects the important features without any human supervision\n",
    "\n",
    "\n",
    "- And it makes efficient model which performs automatic feature extraction to achieve superhuman accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pXZ8NJd0mLGU",
   "metadata": {
    "id": "pXZ8NJd0mLGU"
   },
   "source": [
    "# CNN Model Building Details :\n",
    "- The main building block of CNN is the convolutional layer.\n",
    "\n",
    "- Convolution is a mathematical operation to merge two sets of information. \n",
    "- In our case the convolution is applied on the input data using a convolution filter \n",
    "  to produce a feature map.\n",
    "- After a convolution operation we usually perform pooling to reduce the dimensionality. \n",
    "  This enables us to reduce the number of parameters, \n",
    "  which both shortens the training time and combats overfitting. \n",
    "- Pooling layers downsample each feature map independently, reducing the height and width.\n",
    "\n",
    "- The output of both convolution and pooling layers are 3D volumes, \n",
    "  but a fully connected layer expects a 1D vector of numbers. \n",
    "- So we flatten the output of the final pooling layer to a vector \n",
    "  and that becomes the input to the fully connected layer. \n",
    "- Flattening is simply arranging the 3D volume of numbers into a 1D vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9vmo59mWMr",
   "metadata": {
    "id": "ee9vmo59mWMr"
   },
   "source": [
    "Let's start the coding...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KEJJaZdQmySc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30623,
     "status": "ok",
     "timestamp": 1654361143512,
     "user": {
      "displayName": "Prashant Gaikwad",
      "userId": "13073936513131440156"
     },
     "user_tz": -330
    },
    "id": "KEJJaZdQmySc",
    "outputId": "d28eee3d-5845-4a87-b7e9-3f332ed09e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-reasoning",
   "metadata": {
    "id": "later-reasoning",
    "outputId": "ee233b6a-47bb-4ad4-a6cc-4def606e8749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8369 images belonging to 7 classes.\n",
      "Found 1735 images belonging to 7 classes.\n",
      "WARNING:tensorflow:From <ipython-input-2-57d0b9b35658>:52: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "130/130 [==============================] - 51s 392ms/step - loss: 1.8029 - accuracy: 0.2213 - val_loss: 1.8781 - val_accuracy: 0.1979\n",
      "Epoch 2/100\n",
      "130/130 [==============================] - 53s 404ms/step - loss: 1.7760 - accuracy: 0.2396 - val_loss: 1.8508 - val_accuracy: 0.2043\n",
      "Epoch 3/100\n",
      "130/130 [==============================] - 53s 407ms/step - loss: 1.7250 - accuracy: 0.2936 - val_loss: 1.8139 - val_accuracy: 0.2963\n",
      "Epoch 4/100\n",
      "130/130 [==============================] - 53s 407ms/step - loss: 1.6502 - accuracy: 0.3565 - val_loss: 1.7309 - val_accuracy: 0.3565\n",
      "Epoch 5/100\n",
      "130/130 [==============================] - 53s 408ms/step - loss: 1.5859 - accuracy: 0.3884 - val_loss: 1.6938 - val_accuracy: 0.3686\n",
      "Epoch 6/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 1.5465 - accuracy: 0.4149 - val_loss: 1.6583 - val_accuracy: 0.3727\n",
      "Epoch 7/100\n",
      "130/130 [==============================] - 53s 409ms/step - loss: 1.5159 - accuracy: 0.4301 - val_loss: 1.6263 - val_accuracy: 0.3814\n",
      "Epoch 8/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 1.4763 - accuracy: 0.4459 - val_loss: 1.6032 - val_accuracy: 0.3964\n",
      "Epoch 9/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 1.4541 - accuracy: 0.4524 - val_loss: 1.5927 - val_accuracy: 0.3906\n",
      "Epoch 10/100\n",
      "130/130 [==============================] - 53s 410ms/step - loss: 1.4254 - accuracy: 0.4669 - val_loss: 1.5665 - val_accuracy: 0.4097\n",
      "Epoch 11/100\n",
      "130/130 [==============================] - 53s 410ms/step - loss: 1.3968 - accuracy: 0.4759 - val_loss: 1.5471 - val_accuracy: 0.4091\n",
      "Epoch 12/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 1.3720 - accuracy: 0.4865 - val_loss: 1.5184 - val_accuracy: 0.4138\n",
      "Epoch 13/100\n",
      "130/130 [==============================] - 53s 409ms/step - loss: 1.3463 - accuracy: 0.4928 - val_loss: 1.5322 - val_accuracy: 0.4265\n",
      "Epoch 14/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 1.3208 - accuracy: 0.5111 - val_loss: 1.5038 - val_accuracy: 0.4213\n",
      "Epoch 15/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 1.2920 - accuracy: 0.5235 - val_loss: 1.4857 - val_accuracy: 0.4300\n",
      "Epoch 16/100\n",
      "130/130 [==============================] - 53s 410ms/step - loss: 1.2786 - accuracy: 0.5237 - val_loss: 1.4598 - val_accuracy: 0.4456\n",
      "Epoch 17/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 1.2476 - accuracy: 0.5392 - val_loss: 1.4655 - val_accuracy: 0.4468\n",
      "Epoch 18/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 1.2190 - accuracy: 0.5499 - val_loss: 1.4411 - val_accuracy: 0.4520\n",
      "Epoch 19/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 1.2034 - accuracy: 0.5521 - val_loss: 1.4269 - val_accuracy: 0.4653\n",
      "Epoch 20/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 1.1769 - accuracy: 0.5639 - val_loss: 1.4146 - val_accuracy: 0.4751\n",
      "Epoch 21/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 1.1540 - accuracy: 0.5746 - val_loss: 1.4101 - val_accuracy: 0.4606\n",
      "Epoch 22/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 1.1228 - accuracy: 0.5863 - val_loss: 1.4144 - val_accuracy: 0.4664\n",
      "Epoch 23/100\n",
      "130/130 [==============================] - 54s 415ms/step - loss: 1.1042 - accuracy: 0.5984 - val_loss: 1.4390 - val_accuracy: 0.4630\n",
      "Epoch 24/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 1.0805 - accuracy: 0.6069 - val_loss: 1.3906 - val_accuracy: 0.4734\n",
      "Epoch 25/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 1.0589 - accuracy: 0.6212 - val_loss: 1.3884 - val_accuracy: 0.4850\n",
      "Epoch 26/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 1.0288 - accuracy: 0.6267 - val_loss: 1.3651 - val_accuracy: 0.4867\n",
      "Epoch 27/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 1.0060 - accuracy: 0.6321 - val_loss: 1.3465 - val_accuracy: 0.4896\n",
      "Epoch 28/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.9879 - accuracy: 0.6379 - val_loss: 1.3788 - val_accuracy: 0.4803\n",
      "Epoch 29/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.9597 - accuracy: 0.6494 - val_loss: 1.3635 - val_accuracy: 0.4994\n",
      "Epoch 30/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.9347 - accuracy: 0.6648 - val_loss: 1.3558 - val_accuracy: 0.5012\n",
      "Epoch 31/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.9043 - accuracy: 0.6763 - val_loss: 1.3419 - val_accuracy: 0.5191\n",
      "Epoch 32/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.8848 - accuracy: 0.6780 - val_loss: 1.3595 - val_accuracy: 0.5110\n",
      "Epoch 33/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.8575 - accuracy: 0.6911 - val_loss: 1.3636 - val_accuracy: 0.5069\n",
      "Epoch 34/100\n",
      "130/130 [==============================] - 54s 416ms/step - loss: 0.8293 - accuracy: 0.6989 - val_loss: 1.3915 - val_accuracy: 0.5098\n",
      "Epoch 35/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.8080 - accuracy: 0.7097 - val_loss: 1.3476 - val_accuracy: 0.5093\n",
      "Epoch 36/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.7779 - accuracy: 0.7161 - val_loss: 1.3739 - val_accuracy: 0.5052\n",
      "Epoch 37/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.7513 - accuracy: 0.7262 - val_loss: 1.3577 - val_accuracy: 0.5139\n",
      "Epoch 38/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.7249 - accuracy: 0.7412 - val_loss: 1.3775 - val_accuracy: 0.5104\n",
      "Epoch 39/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.7065 - accuracy: 0.7495 - val_loss: 1.3903 - val_accuracy: 0.5191\n",
      "Epoch 40/100\n",
      "130/130 [==============================] - 54s 415ms/step - loss: 0.6841 - accuracy: 0.7564 - val_loss: 1.3837 - val_accuracy: 0.5179\n",
      "Epoch 41/100\n",
      "130/130 [==============================] - 54s 415ms/step - loss: 0.6593 - accuracy: 0.7604 - val_loss: 1.4045 - val_accuracy: 0.5237\n",
      "Epoch 42/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.6323 - accuracy: 0.7754 - val_loss: 1.4315 - val_accuracy: 0.5041\n",
      "Epoch 43/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.6210 - accuracy: 0.7769 - val_loss: 1.4891 - val_accuracy: 0.5145\n",
      "Epoch 44/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.5962 - accuracy: 0.7863 - val_loss: 1.4085 - val_accuracy: 0.5139\n",
      "Epoch 45/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.5674 - accuracy: 0.8041 - val_loss: 1.4298 - val_accuracy: 0.5110\n",
      "Epoch 46/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.5470 - accuracy: 0.8054 - val_loss: 1.4699 - val_accuracy: 0.5214\n",
      "Epoch 47/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.5280 - accuracy: 0.8130 - val_loss: 1.4890 - val_accuracy: 0.5116\n",
      "Epoch 48/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.5063 - accuracy: 0.8183 - val_loss: 1.4703 - val_accuracy: 0.5139\n",
      "Epoch 49/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.4829 - accuracy: 0.8281 - val_loss: 1.5035 - val_accuracy: 0.5197\n",
      "Epoch 50/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.4705 - accuracy: 0.8315 - val_loss: 1.5458 - val_accuracy: 0.5179\n",
      "Epoch 51/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.4521 - accuracy: 0.8403 - val_loss: 1.5687 - val_accuracy: 0.5260\n",
      "Epoch 52/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.4392 - accuracy: 0.8406 - val_loss: 1.5445 - val_accuracy: 0.5150\n",
      "Epoch 53/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.4260 - accuracy: 0.8509 - val_loss: 1.5572 - val_accuracy: 0.5133\n",
      "Epoch 54/100\n",
      "130/130 [==============================] - 53s 410ms/step - loss: 0.3959 - accuracy: 0.8618 - val_loss: 1.5523 - val_accuracy: 0.5203\n",
      "Epoch 55/100\n",
      "130/130 [==============================] - 53s 410ms/step - loss: 0.3866 - accuracy: 0.8632 - val_loss: 1.5631 - val_accuracy: 0.5255\n",
      "Epoch 56/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.3722 - accuracy: 0.8719 - val_loss: 1.5954 - val_accuracy: 0.5208\n",
      "Epoch 57/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.3674 - accuracy: 0.8712 - val_loss: 1.6101 - val_accuracy: 0.5226\n",
      "Epoch 58/100\n",
      "130/130 [==============================] - 54s 416ms/step - loss: 0.3505 - accuracy: 0.8800 - val_loss: 1.5901 - val_accuracy: 0.5260\n",
      "Epoch 59/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.3270 - accuracy: 0.8881 - val_loss: 1.6540 - val_accuracy: 0.5226\n",
      "Epoch 60/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.3224 - accuracy: 0.8861 - val_loss: 1.6382 - val_accuracy: 0.5237\n",
      "Epoch 61/100\n",
      "130/130 [==============================] - 54s 417ms/step - loss: 0.3121 - accuracy: 0.8909 - val_loss: 1.6831 - val_accuracy: 0.5226\n",
      "Epoch 62/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.3042 - accuracy: 0.8901 - val_loss: 1.7356 - val_accuracy: 0.5214\n",
      "Epoch 63/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.2934 - accuracy: 0.8973 - val_loss: 1.7430 - val_accuracy: 0.5191\n",
      "Epoch 64/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.2871 - accuracy: 0.9009 - val_loss: 1.6975 - val_accuracy: 0.5203\n",
      "Epoch 65/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.2692 - accuracy: 0.9042 - val_loss: 1.7123 - val_accuracy: 0.5156\n",
      "Epoch 66/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.2634 - accuracy: 0.9070 - val_loss: 1.8001 - val_accuracy: 0.5208\n",
      "Epoch 67/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.2531 - accuracy: 0.9150 - val_loss: 1.7789 - val_accuracy: 0.5145\n",
      "Epoch 68/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.2518 - accuracy: 0.9115 - val_loss: 1.7574 - val_accuracy: 0.5214\n",
      "Epoch 69/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.2484 - accuracy: 0.9167 - val_loss: 1.8202 - val_accuracy: 0.5116\n",
      "Epoch 70/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.2368 - accuracy: 0.9186 - val_loss: 1.8586 - val_accuracy: 0.5197\n",
      "Epoch 71/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.2294 - accuracy: 0.9228 - val_loss: 1.8448 - val_accuracy: 0.5179\n",
      "Epoch 72/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.2162 - accuracy: 0.9281 - val_loss: 1.8263 - val_accuracy: 0.5220\n",
      "Epoch 73/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.2090 - accuracy: 0.9255 - val_loss: 1.8557 - val_accuracy: 0.5220\n",
      "Epoch 74/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.2050 - accuracy: 0.9267 - val_loss: 1.8554 - val_accuracy: 0.5162\n",
      "Epoch 75/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1953 - accuracy: 0.9347 - val_loss: 1.9319 - val_accuracy: 0.5203\n",
      "Epoch 76/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1902 - accuracy: 0.9341 - val_loss: 1.8877 - val_accuracy: 0.5266\n",
      "Epoch 77/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1869 - accuracy: 0.9352 - val_loss: 1.9064 - val_accuracy: 0.5197\n",
      "Epoch 78/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.1904 - accuracy: 0.9358 - val_loss: 1.8593 - val_accuracy: 0.5341\n",
      "Epoch 79/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.1727 - accuracy: 0.9439 - val_loss: 1.9518 - val_accuracy: 0.5220\n",
      "Epoch 80/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.1703 - accuracy: 0.9430 - val_loss: 1.9770 - val_accuracy: 0.5260\n",
      "Epoch 81/100\n",
      "130/130 [==============================] - 54s 416ms/step - loss: 0.1712 - accuracy: 0.9397 - val_loss: 1.9127 - val_accuracy: 0.5272\n",
      "Epoch 82/100\n",
      "130/130 [==============================] - 55s 420ms/step - loss: 0.1688 - accuracy: 0.9428 - val_loss: 1.9767 - val_accuracy: 0.5330\n",
      "Epoch 83/100\n",
      "130/130 [==============================] - 55s 423ms/step - loss: 0.1744 - accuracy: 0.9411 - val_loss: 1.9880 - val_accuracy: 0.5208\n",
      "Epoch 84/100\n",
      "130/130 [==============================] - 54s 419ms/step - loss: 0.1582 - accuracy: 0.9457 - val_loss: 1.9456 - val_accuracy: 0.5341\n",
      "Epoch 85/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1504 - accuracy: 0.9475 - val_loss: 2.0040 - val_accuracy: 0.5214\n",
      "Epoch 86/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.1427 - accuracy: 0.9532 - val_loss: 2.0154 - val_accuracy: 0.5220\n",
      "Epoch 87/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.1571 - accuracy: 0.9451 - val_loss: 2.0098 - val_accuracy: 0.5214\n",
      "Epoch 88/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1360 - accuracy: 0.9550 - val_loss: 2.1015 - val_accuracy: 0.5214\n",
      "Epoch 89/100\n",
      "130/130 [==============================] - 54s 413ms/step - loss: 0.1424 - accuracy: 0.9526 - val_loss: 2.0344 - val_accuracy: 0.5347\n",
      "Epoch 90/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1301 - accuracy: 0.9568 - val_loss: 2.0435 - val_accuracy: 0.5301\n",
      "Epoch 91/100\n",
      "130/130 [==============================] - 53s 412ms/step - loss: 0.1377 - accuracy: 0.9554 - val_loss: 2.0759 - val_accuracy: 0.5214\n",
      "Epoch 92/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.1374 - accuracy: 0.9530 - val_loss: 2.0590 - val_accuracy: 0.5220\n",
      "Epoch 93/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1284 - accuracy: 0.9570 - val_loss: 2.1078 - val_accuracy: 0.5214\n",
      "Epoch 94/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.1271 - accuracy: 0.9581 - val_loss: 2.1593 - val_accuracy: 0.5278\n",
      "Epoch 95/100\n",
      "130/130 [==============================] - 54s 414ms/step - loss: 0.1197 - accuracy: 0.9586 - val_loss: 2.1839 - val_accuracy: 0.5266\n",
      "Epoch 96/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1143 - accuracy: 0.9618 - val_loss: 2.1670 - val_accuracy: 0.5249\n",
      "Epoch 97/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1159 - accuracy: 0.9618 - val_loss: 2.2032 - val_accuracy: 0.5243\n",
      "Epoch 98/100\n",
      "130/130 [==============================] - 53s 411ms/step - loss: 0.1154 - accuracy: 0.9626 - val_loss: 2.2198 - val_accuracy: 0.5168\n",
      "Epoch 99/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1112 - accuracy: 0.9644 - val_loss: 2.1350 - val_accuracy: 0.5185\n",
      "Epoch 100/100\n",
      "130/130 [==============================] - 54s 412ms/step - loss: 0.1127 - accuracy: 0.9634 - val_loss: 2.2159 - val_accuracy: 0.5214\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam #optimizer is use to minimize the loss function\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize image data generator with rescaling, this is the augmentation configuration we will use for training data, It generates more images using below parameters\n",
    "#Rescale : One of many augmentation parameters, adjusts the pixel values of our image,\n",
    "#Setting rescale = 1./255 will adjust our pixel values to be between 0–1.\n",
    "\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all test images\n",
    "# this is a generator that will read pictures found at train_data_path, and indefinitely generate batches of augmented image data\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        '/content/drive/MyDrive/Almabetter/Data Science Capstone Projects/Live Class Monitoring System [Face emotion Recognition] - Prashant Gaikwad/data/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Preprocess all train images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        '/content/drive/MyDrive/Almabetter/Data Science Capstone Projects/Live Class Monitoring System [Face emotion Recognition] - Prashant Gaikwad/data/test',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "# create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "# Train the neural network/model\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= 8369 // 64,\n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= 1735 // 64)\n",
    "\n",
    "# save model structure in jason file\n",
    "model_json = emotion_model.to_json()\n",
    "with open(\"/content/drive/MyDrive/Almabetter/Data Science Capstone Projects/Live Class Monitoring System [Face emotion Recognition] - Prashant Gaikwad/model/emotion_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save trained model weight in .h5 file\n",
    "emotion_model.save_weights('/content/drive/MyDrive/Almabetter/Data Science Capstone Projects/Live Class Monitoring System [Face emotion Recognition] - Prashant Gaikwad/model/emotion_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-midwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"model/emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# pass here your video path\n",
    "cap = cv2.VideoCapture(\"sample video/sample zoom recording.mp4\")\n",
    "\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces available on camera\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # take each face available on the camera and Preprocess it\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-cloud",
   "metadata": {},
   "source": [
    "<img src=\"images/Screenshot (449).png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-boards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"model/emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces available on camera\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # take each face available on the camera and Preprocess it\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-collect",
   "metadata": {},
   "source": [
    "<img src=\"images/Screenshot (453).png\">\n",
    "<img src=\"images/Screenshot (456).png\">\n",
    "<img src=\"images/Screenshot (457).png\">\n",
    "<img src=\"images/Screenshot (458).png\">\n",
    "<img src=\"images/Screenshot (460).png\">\n",
    "<img src=\"images/Screenshot (465).png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9owBhB39qSu7",
   "metadata": {
    "id": "9owBhB39qSu7"
   },
   "source": [
    "# Evaluate Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-horizon",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 750
    },
    "executionInfo": {
     "elapsed": 406369,
     "status": "ok",
     "timestamp": 1654364188906,
     "user": {
      "displayName": "Prashant Gaikwad",
      "userId": "13073936513131440156"
     },
     "user_tz": -330
    },
    "id": "plain-horizon",
    "outputId": "baad0f72-44c8-477f-9e99-91faf0b10e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Found 1735 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "[[ 45   8   5  60  67 107  40]\n",
      " [ 10   2   8  18  18  22  13]\n",
      " [ 19   4   1  23  27  36  20]\n",
      " [ 43   4   5  58  52  86  29]\n",
      " [ 33   8   5  61  63 123  49]\n",
      " [ 33  10   6  76  76 102  43]\n",
      " [ 31   7   6  48  40  55  30]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.14      0.16       332\n",
      "           1       0.05      0.02      0.03        91\n",
      "           2       0.03      0.01      0.01       130\n",
      "           3       0.17      0.21      0.19       277\n",
      "           4       0.18      0.18      0.18       342\n",
      "           5       0.19      0.29      0.23       346\n",
      "           6       0.13      0.14      0.14       217\n",
      "\n",
      "    accuracy                           0.17      1735\n",
      "   macro avg       0.14      0.14      0.14      1735\n",
      "weighted avg       0.16      0.17      0.16      1735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('/content/drive/MyDrive/Almabetter/Data Science Capstone Projects/Live Class Monitoring System [Face emotion Recognition] - Prashant Gaikwad/model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"/content/drive/MyDrive/Almabetter/Data Science Capstone Projects/Live Class Monitoring System [Face emotion Recognition] - Prashant Gaikwad/model/emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# Initialize image data generator with rescaling\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all test images\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "        '/content/drive/MyDrive/Almabetter/Data Science Capstone Projects/Live Class Monitoring System [Face emotion Recognition] - Prashant Gaikwad/data/test',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "# do prediction on test data\n",
    "predictions = emotion_model.predict_generator(test_generator)\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "# confusion matrix\n",
    "c_matrix = confusion_matrix(test_generator.classes, predictions.argmax(axis=1))\n",
    "print(c_matrix)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=c_matrix, display_labels=emotion_dict)\n",
    "cm_display.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(classification_report(test_generator.classes, predictions.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TFK_REUGFiiG",
   "metadata": {
    "id": "TFK_REUGFiiG"
   },
   "source": [
    "The confusion matrix clearly shows that our model is doing good job on the class Happy, Neutral, Sad, Surprised, Angry, but it's performance is low on Fearful and Disgusted class. \n",
    "\n",
    "One of the reason for this could be the fact that these two classes have less data. But when I looked at the images I found some images from these two classes are even hard for a human to tell whether the person is sad or neutral. Facial expression depends on individual as well. Some person's neutral face looks more like Fearful and Disgusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73w2ySaUyzpW",
   "metadata": {
    "id": "73w2ySaUyzpW"
   },
   "source": [
    "# Conclusion:\n",
    "\n",
    "In this Project, we have seen how to preprocess image data, design a network that is capable of classifying the emotions, and then use Opencv\n",
    "for the detection of the faces and then pass it for prediction.\n",
    "\n",
    "The Project aims to create a system that automatically supports teachers and related education.\n",
    "\n",
    "Aims to create a system that automatically supports teachers and skills related to monitoring student behavior. \n",
    "\n",
    "The system will serve as an important factor in decision-making processes. \n",
    "\n",
    "The results of the emotion detection algorithm gave average\n",
    "accuracy up to 96% for Convolutional Neural Network model.\n",
    "\n",
    "The confusion matrix clearly shows that our model is doing good job on the class Happy, Neutral, Sad, Surprised, Angry, but it's performance is low on Fearful and Disgusted class. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Notebook05_Prashant_Gaikwad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
